# -*- coding: utf-8 -*-
"""LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kwbN6pkqrX0vM4x-1O6JipL3V0_p2zzR

#I. Importing the Library
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Terminal Colors
RED = "\033[31;1m"
GREEN = "\033[32;1m"
YELLOW = "\033[33;1m"
BLUE = "\033[34;1m"
PURPLE = "\033[35;1m"
CYAN = "\033[36;1m"
WHITE = "\033[37;1m"
ORANGE = "\033[38;5;208m"
RESET = "\033[0;0m"

"""#II. Loading the dataset"""

filepath="/content/drive/MyDrive/heart.csv"

heart_df = pd.read_csv(filepath) #Reads the csv file where we will be extracting the data, analysing and manipulating it

heart_df.head()
# Displays the first few rows of the dataset to check that it loaded correctly.

for col in heart_df.columns: # Loops through every column name in the dataset.
  if heart_df[col].dtype == "object":
    # Checks whether the current column is a categorical column (stored as 'object').
    print(f"{col}--{heart_df[col].nunique()}") # Prints the column name and how many unique categories it has.
    display(heart_df[col].unique())# Shows all the unique category values that appear in that column.
    print("==============================")

"""^for loop goes through each column of the dataframe. specifically for categorical data will carry on to to the if statement. " heart_df[col].nunique() "counts how many distinct values the coulmn has.  "display(heart_df[col].unique())" shows all the unique, indivdual values in that clumn.  overall purpose

#III. Feature Engineering
"""

from sklearn.model_selection import train_test_split
# used to divide the dataset into graning anf testing, so the prediction model can be trained on one portion of the data and evaluated on the unseesn data, this will be useful for then seeing the odels accuracy and performance

from sklearn.preprocessing import StandardScaler
#StandardScaler is used to standardize numerical features by removing the mean and scaling to unit variance. improving the performance


#sk learn used for just evaluation purposed to test and train data
#z score - standard scaler, pushing values towards mean
#data is normally distributed -bimodal

#One-Hot Encoding for the categorical data
heart_df_ohc = pd.get_dummies(heart_df, prefix = "")
heart_df_ohc.head()


#Converts categorical variables in heart_df into one-hot encoded columns.
#One-hot encoding creates new binary columns (0 or 1) for each category in a categorical feature. -  Useful when seeing who has a case of heart disease and who doesnt
#.head() just prints oiut to show results

X = heart_df_ohc.drop(labels=["HeartDisease"], axis = 1).values.astype(float)
# Creates the feature matrix X by removing the 'HeartDisease' column.
# All remaining columns are used as input features.
# .values converts the DataFrame to a NumPy array and .astype(float) ensures all values are numeric.

y = heart_df_ohc["HeartDisease"].values
# Extracts the 'HeartDisease' column as the target variable y.
# .values converts it into a NumPy array for model training.

X_train, X_test,y_train, y_test = train_test_split(X,y, test_size=0.225, random_state=10)
# Splits the data into training and testing sets.
# 22.5% of the data is used for testing, and the remaining 77.5% is used for training.
# random_state=10 ensures the split is reproducible so the results stay consistent each time the code runs.

print(X_train.shape ,X_test.shape,y_train.shape, y_test.shape )
# Prints the dimensions of the training and testing sets.
# This helps verify that the data has been split correctly into features (X) and labels (y).

standardise = StandardScaler()
# Creates a StandardScaler object which will normalise the feature data.
# It scales each feature so it has a mean of 0 and a standard deviation of 1, improving model performance.

X_train[:, :6]  = standardise.fit_transform(X_train[:, :6])
# Standardises the first six columns of the training data.
# The scaler learns the average and spread of these columns (fit),
# then applies the scaling to convert them into standardised values (transform).

X_test[:, :6] = standardise.transform(X_test[:, :6])
#categorical data - not types of transformation as not number

# Applies the same scaling learned from the training data to the test data.
# This ensures both sets are scaled consistently without refitting on the test data.

display(X_train[:5])
# Shows the first five rows of the training data to verify that the scaling was applied correctly.

"""#IV. Exploratory Data Analysis

## I. Count Plot
"""

import matplotlib.pyplot as plt
import seaborn as sns

#checking if it is a balanced/unbalanced datset
#libraries for observatory

#set the figure size
plt.figure(figsize=(8,6))

#set a seaborn style for max user experience
sns.set_theme(style="darkgrid")

ax = sns.countplot(heart_df, x="Sex", hue="HeartDisease", palette=["#4CA952", "#710A35"])
# Creates a count plot on X variable, showing how many males and females are in the dataset
# with separate colours indicating whether each person has heart disease or not.
# The 'hue' parameter splits the bars by HeartDisease, and 'palette' sets the colours used.
#--

#Adding counts on the top of the bar
for value in ax.containers:
  ax.bar_label(value, fmt="%d", label_type="edge",padding=5)
# fmt="%d" displays the numbers as integers, label_type="edge" places them at the top, and padding=5 adds space above the bars.


#Add title and labels and axis labels of the plot with custom font sizes and bold title.
plt.title("Distribution of Heart Ailment as per Gender", fontsize=15, fontweight="bold")
plt.xlabel("Gender", fontsize=12)
plt.ylabel("Count", fontsize=12)

# Clarify legend labels
handles, labels = ax.get_legend_handles_labels()

new_labels = ['No Heart Disease' if label == '0' else 'Heart Disease' for label in labels]
ax.legend(handles=handles, labels=new_labels, title="Heart Disease")
# '0' means 'No Heart Disease' and '1' means 'Heart Disease', and the legend title is set accordingly.
# Replace numeric legend labels with descriptive text for clarity.


plt.grid(visible=True, linestyle="--", linewidth=0.5, alpha=0.5)
# Adds gridlines to the plot to make it easier to read values.
# Dashed lines (linestyle="--"), thin width (linewidth=0.5), and slight transparency (alpha=0.5) are plot formatting to improve readability without cluttering the plot.


plt.show()
#Displays the plot in the output to visually inspect the data.

#YAY BALANCED DATASET - Not imbalanced

"""#II. KDE Plot"""

#Plotting data in a normal distriution pattern
#only applicable for categorical data - checking distribution

#had to remove heart disease column so create d a list- immutable so 2 step process


columns = list(heart_df_ohc.columns) # tuple - therefore immutable
columns.remove("HeartDisease")
# Create a list of column names excluding 'HeartDisease'.
# Lists are mutable, so we can remove a column, whereas heart_df_ohc.columns is immutable and cannot be modified directly.

kde_df = pd.concat(
  [
    pd.DataFrame(X_train, columns=columns),
    pd.DataFrame(X_test, columns=columns)

  ]
)
# Combines the training and testing feature data into a single DataFrame.
# This is useful for plotting or analysis across the full dataset while keeping column names.

#Set a figure size
plt.figure(figsize=(8,6))

#Set a seaborn-style
sns.set_theme(style="darkgrid")

#Create a KDE plot

for col in kde_df.columns:
  sns.kdeplot(data= kde_df[col], fill=True, bw_adjust=1, label=col)
# Plots Kernel Density Estimates (KDE) for each feature column in kde_df.
# KDE shows the distribution of data values for each feature, with fill=True shading under the curve.
# bw_adjust=1 controls the smoothness of the curve, and label=col adds a label for the legend.



#Add title and labels of the KDE plot with custom font sizes and bold title for clarity.
plt.title("Kernel Density Estimation of Features", fontsize=15, fontweight="bold")
plt.xlabel("Features", fontsize=12)
plt.ylabel("Density", fontsize=12)

#Add legend
plt.legend(loc="upper left")
# Adds a key (legend) to the plot so we know which curve corresponds to each feature.
#from this I infered that the distribution is bimodal for many features - 2 peaks, 2 most freq values

# Limits the graph's x-axis range of the plot from -3 to 3 to focus on the main data distribution.
plt.xlim(-3,3)


plt.grid(visible=True, linestyle="--", linewidth=0.5, alpha=0.5)
# Adds gridlines to the plot to make it easier to read values.
# Dashed lines (linestyle="--"), thin width (linewidth=0.5), and slight transparency (alpha=0.5) are plot formatting to improve readability without cluttering the plot.


plt.show()
#Displays the plot in the output to visually inspect the data.

"""##III. Heatmap Plot"""

column = heart_df_ohc['HeartDisease']
#error- -1 from total no of columns, err""unbounded slice"
#heart_df_ohc.insert(-1, column="Heart Disease", value=column)
heart_df_ohc.insert(21, column="Heart Disease", value=column)
heart_df_ohc.drop(columns=["HeartDisease"], axis=1, inplace=True)
heart_df_ohc.head()
'''
This block of code first extracts the original HeartDisease column from the DataFrame so it can be repositioned. The commented-out line shows an attempt to insert the column at the end using -1, which caused an “unbounded slice” error. To fix this, the column is inserted explicitly at position 21 with a new name, Heart Disease, to reorder it for clarity or consistency. The original HeartDisease column is then removed to avoid duplicates. Finally, head() is used to display the first five rows, verifying that the column has been successfully reordered.
'''

#Set the Figure size
plt.figure(figsize=(20,15))

#set seaborn theme
sns.set_theme(style="darkgrid")

#Correlation Heatmap
corr = heart_df_ohc.corr()

sns.heatmap(corr, annot=True, fmt=".0%", cmap="copper", linewidth=1, linecolor="#FFF")

#Show title
plt.title("Heatmap Correlation analysis of all features", fontsize=15, fontweight="bold")

#Rotation hz.ticks
plt.xticks(rotation=30, fontsize=12)



plt.show()
#low/mild correlation between features -

'''
This block of code visualises the correlation between all features in the dataset. First, the figure size is set to make the plot large and readable, and a Seaborn theme with a dark grid is applied for better aesthetics. The correlation matrix is computed using .corr(), which calculates pairwise correlations between features. A heatmap is then plotted to display these correlations, with annotations showing the correlation values as percentages, a copper colour map for clarity, and white gridlines separating the cells. The plot title is added with a larger, bold font, and the x-axis tick labels are rotated for readability. Finally, plt.show() displays the heatmap. From the plot, we can observe that the correlations between features are generally low to mild.
'''

"""# V. Logistic Regression from Scratch"""

from sklearn.metrics import confusion_matrix, classification_report
'''
This line imports two evaluation tools from scikit-learn: confusion_matrix and classification_report. The confusion_matrix shows the number of correct and incorrect predictions for each class, helping to visualise how well the model is performing. The classification_report provides detailed metrics such as precision, recall, F1-score, and support for each class, giving a comprehensive assessment of model performance.
'''

class LogisticRegression:
  '''
  Mathematical Idea:-
  --------------------
  1.Hypothesis: h(x)= σ(Z) where Z = w*T.X +b
  2. Sigmoid Function: σ(z) = 1/1+e^(-z)
  3. Cost Function: J(w,b) = -1/m *∑[y* log(h) + (1-y) * log(1-h)]
  4. Gradients:
      -δJ/δw = 1/m * X^T *(h-y)
      -δJ/δw = 1/m * ∑(h-y)
  '''
# This LogisticRegression class predicts the probability of an outcome,
# such as whether someone has heart disease. It calculates a value Z = w^T X + b,
# which combines all input features X with their weights w and a bias b.
# The sigmoid function σ(z) = 1 / (1 + e^(-z)) converts Z into a probability between 0 and 1.
# The model then compares these predictions to the actual results using a cost function whch refers to how far away from the predicted to real value,
# and gradually updates w and b using gradient descent to make more accurate predictions.



  #creating initialisation function
  def __init__(self, learning_rate=0.01, n_iterations=1000, verbose=True):
    self.learning_rate = learning_rate
    self.n_iterations = n_iterations
    self.verbose = verbose
    self.weights = None
    self.bias = None
    self.cost_history = []

'''
# Initializes the LogisticRegression model with given parameters:
# learning_rate controls how big each update step is,
# n_iterations sets how many times the model updates weights,
# verbose decides if progress is printed.
# weights and bias are initially None and will be set during training,
# cost_history will store the cost at each iteration for analysis.

'''

  #private func
  def _sigmoid(self,z):
    #clip z to prevents overflow in exp
    z = np.clip(z, -500,500)
    return 1/(1 + np.exp(-z))
# Private method to calculate the sigmoid function, σ(z) = 1 / (1 + e^(-z)).
# The input z is clipped between -500 and 500 to prevent numerical overflow in the exponential.
# Converts any value of z into a probability between 0 and 1.


  def _compute_cost(self, y_true, y_pred):

    #Compute the bnary cross-entropy loss (log loss)
    # J(w,b) = -1/m *∑[y* log(h) + (1-y) * log(1-h)]

    m = len(y_true)
'''
# Computes the cost (error) of the model using binary cross-entropy (log loss).
# Formula: J(w, b) = -1/m * ∑[y * log(h) + (1-y) * log(1-h)]
# m is the number of samples, y_true are the actual labels, and y_pred are the predicted probabilities.

'''

  #add small epsilon to prevent log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1-epsilon)


  #binary cross-entropy loss
    cost = -1/m * np.sum(
      y_true * np.log(y_pred) + (1-y_true) * np.log(1-y_pred)
      )
    return cost

'''
for both prev snippets:
  # Adds a small value (epsilon) to y_pred to prevent taking log(0), which is undefined.
# Calculates the binary cross-entropy loss (log loss) for all samples.
# This measures how well the predicted probabilities match the actual labels.
# Returns the average cost across all samples.

'''


  #----------
  def _compute_gradients(self, X, y_true, y_pred):
    '''
    Compute gradients for the weight and bias
    -δJ/δw = 1/m * X^T *(h-y)
    δJ/δw = 1/m * ∑(h-y)
    '''


    m = X.shape[0]

    #error term
    error = y_pred - y_true

    #gradients
    dw = (1/m) * np.dot(X.T, error)
    db = (1/m) * np.sum(error)
    return dw,db

   ]

   '''
   # Computes the gradients of the cost function with respect to the weights (dw) and bias (db).
# The error term (y_pred - y_true) shows how far off the predictions are.
# dw = 1/m * X^T * (error) calculates how much each weight should change.
# db = 1/m * sum(error) calculates how much the bias should change.
# These gradients are used to update the weights and bias during training.

   '''

  def fit(self, X, y):
  #Convert to numpy arrays
    X = np.array(X, dtype=float)
    y = np.array(y).reshape(-1)

# Converts the input features X and labels y into NumPy arrays.
# Ensures X is of type float for calculations and y is reshaped into a 1D array for consistency.


    #Get dimensions
    m , n = X.shape
    # m is the number of samples and n is the number of features.

    #Initialise parameters
    self.weights = np.zeros(n)
    self.bias=0
    self.cost_history = []
    # Weights are set to zero initially, bias is zero, and cost_history will store the cost at each iteration.

    #gradient descent
    for i in range(self.n_iterations):
      #forward propagations
      z = np.dot(X, self.weights) + self.bias
      y_pred = self. _sigmoid(z)
      # Forward propagation: calculate the linear combination of inputs and weights (z)
    # and apply the sigmoid function to get predicted probabilities (y_pred) for each sample.


      # Compute the cost (error) for the current predictions and store it.
# This allows tracking how the model's performance improves over iterations.
      cost = self._compute_cost(y, y_pred)
      self.cost_history.append(cost)

      #compute gradients
      dw, db = self._compute_gradients(X, y, y_pred)
      # Compute the gradients of the cost with respect to the weights (dw) and bias (db).
# These show how much the weights and bias need to be adjusted to reduce error.


      #update parameters
      self.weights -= self.learning_rate * dw
      self.bias -= self.learning_rate * db
# Update the weights and bias using gradient descent.
# Each parameter is adjusted by the learning rate multiplied by its gradient to reduce the cost.


      # Print training progress every 100 iterations (or on the final iteration)
# This helps monitor how the cost is decreasing during training.
      if self.verbose and (i % 100 == 0 or i == self.n_iterations-1):
        print(f"Iteration {i:4d}/ |Cost: {cost:6f}")

    return self
#retuns value

    #prediction of probabiity

  def predict_prob(self, X):
    X = np.array(X)
    z = np.dot(X, self.weights) + self.bias
    return self._sigmoid(z)
  # This function estimates how likely each example is to have heart disease.
    # It combines the input values using the model’s learned weights,
    # then uses the sigmoid function to convert that result into a probability between 0 and 1.

  def predict(self, X, threshold=0.5):
    probabilities = self.predict_prob(X)
    return (probabilities >= threshold).astype(int)

  # This function turns predicted probabilities into final yes/no predictions.
    # If the probability is above the threshold (usually 0.5), it predicts 1 (disease),
    # otherwise it predicts 0 (no disease).

  def score(self, X, y):
    y_pred = self.predict(X)
    return np.mean(y_pred == y)  #function asking to show how any values are equal to the eaxct mean

import matplotlib.pyplot as plt
import numpy as np

def plot_decision_boundary(X,y, model, title="Decision Boundary"): # Fixed typo in default title
  '''
  Plot decision boundary for 2D features (using first two dimensions of X)
  '''

  #create mesh
  h = 0.02 #step size
  # Corrected min/max calculation for the first two features
  x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
  y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1


  xx, yy = np.meshgrid(
      np.arange(x_min, x_max, h),
      np.arange(y_min, y_max, h)
  )

  # Prepare a dummy X for prediction for the meshgrid, filling other features with their means
  # This is a common simplification for high-dimensional decision boundary plotting
  # The actual decision boundary is in 20D space, but we are projecting it to 2D
  dummy_X = np.zeros((xx.ravel().shape[0], X.shape[1]))
  dummy_X[:, 0] = xx.ravel()
  dummy_X[:, 1] = yy.ravel()
  # Fill other features with their mean values from the training data
  for i in range(2, X.shape[1]):
      dummy_X[:, i] = X[:, i].mean()

  # predict based on mesh
  Z = model.predict_prob(dummy_X)
  Z = Z.reshape(xx.shape)


  #Plot
  plt.figure(figsize=(10,8))
  plt.contourf(xx, yy, Z, alpha=0.4, cmap="RdYlBu", levels=20) # Corrected colormap name
  plt.colorbar(label="Probability")

  #Plot the data points for the first two features
  scatter = plt.scatter(X[:,0], X[:,1], c=y, cmap="icefire",
  edgecolor="black", s=50, alpha=0.8)
  # Use descriptive labels for the first two features from the `columns` list
  # Assuming `columns` variable (from global scope) holds feature names
  plt.xlabel(columns[0]) # Assuming 'Age' is the first feature
  plt.ylabel(columns[1]) # Assuming 'RestingBP' is the second feature
  plt.title(title)
  plt.legend(*scatter.legend_elements(), title="Classes")
  plt.tight_layout()
  plt.show()

from sklearn.metrics._plot.confusion_matrix import confusion_matrix

def plot_confusion_matrix(y_true, y_pred):
  cm=confusion_matrix(y_true, y_pred)

  plt.figure(figsize=(8,6))
  sns.heatmap(cm, annot=True, fmt="d", cmap="copper", square=True, linewidths=1, linecolor="black")
  plt.xlabel("Predicted label", fontsize=12)
  plt.ylabel("True Label", fontsize=14)
  plt.title("Confusion Matrix", fontsize=15, fontweight="bold")
  plt.tight_layout()
  plt.show()

def model_traning():
  print(f"{YELLOW}","=" * 70)
  print(f"{PURPLE} Logistic Regression from Scratch on heart/stroke datset")
  print(f"{YELLOW}","=" * 70)

  #Split data
  print(f"{BLUE}Training Data Points: {X_train.shape[0]}")
  print(f"{BLUE}Testing Data Points: {X_test.shape[0]}")

  #Training the model
  print(f"\n{GREEN} Training Logistic regression model")
  model = LogisticRegression(learning_rate=0.1, n_iterations=1000, verbose=True)
  model.fit(X_train, y_train)

  return model

def model_predictions(model:LogisticRegression, X_train:np.ndarray, X_test:np.ndarray):
  #making predictions
  print(f"{CYAN}Making predictions ")
  y_train_pred = model.predict(X_train)
  y_test_pred = model.predict(X_test)

  return (y_train_pred, y_test_pred)

from sklearn.metrics import classification_report

def metric_eval_vis(model:LogisticRegression, X:np.ndarray, y_actual:np.ndarray, y_pred:np.ndarray, mode="test"):

    #Calculate accuracy
  accuracy = model.score(X, y_actual)
  print(f"{GREEN} {mode} Accuracy:{accuracy:.2%}") # Fixed format specifier
  print(classification_report(y_actual, y_pred))
  print(f"{BLUE}Classification Report:")

  print(f"{WHITE}", "-" * 70)



  #Model parameters
  print(f"{PURPLE}Model Parameters")
  print(f"{GREEN} Weights:{model.weights}")
  print(f"{GREEN} Bias:{model.bias:6f}")

  if mode == "test": # Added colon and corrected indentation
    #visualisations
    print(f"{WHITE}Generating Visualisations:")

    #Plot Decision Boundary (corrected call)
    plot_decision_boundary(X_train, y_train, model, title="Decision Boundary (Training Set)")

    #plot confusion matrix
    plot_confusion_matrix(y_actual, y_pred)

  print("\n" +f"{YELLOW}" + "=" * 70)

metric_eval_vis(model=model, X=X_test,y_actual=y_test, y_pred=y_test_pred)

"""----------
#VI.Running the Model
-----------
"""

#Training
model = model_traning()

y_train_pred, y_test_pred = model_predictions(model=model, X_train=X_train, X_test=X_test)

display(y_train_pred[:5])
display(y_test_pred[:5])

metric_eval_vis(model=model, X=X_train, y_actual=y_train, y_pred=y_train_pred, mode="train")

