# -*- coding: utf-8 -*-
"""K_ProximityNeighbours.jpynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LZ889IFlJoGW0dCqOI2XikxynvCdZX2m

#Introduction

Try to develop an algorithm that is:-
1. **Instance-based** – uses stored data points directly, no explicit training model.
2. **Non-parametric** – no fixed form/parameters assumed for the function.
3. **Distance-based** – decisions rely on a distance metric (e.g., Euclidean).
4. **Local** – only nearby (k-nearest) points influence the prediction.
5. **Lazy-learning** – “learns” at query time, does almost nothing upfront.
"""

import numpy as np
import pandas as pd
from collections import Counter
from typing import Any, Dict, List, Optional, Tuple

# Terminal Colors
RED = "\033[31;1m"
GREEN = "\033[32;1m"
YELLOW = "\033[33;1m"
BLUE = "\033[34;1m"
PURPLE = "\033[35;1m"
CYAN = "\033[36;1m"
WHITE = "\033[37;1m"
ORANGE = "\033[38;5;208m"
TEAL = "\033[38;5;37m"
RESET = "\033[0;0m"

"""##II. Distance Metrics"""

def euclidean_distance(x1:np.ndarray,x2:np.ndarray)-> int|float:

#Calculate the euclidean distance between two points.
#Formula: d=sqrt((x1_i - x2_i)^2)


  return np.sqrt(np.sum((x1-x2)**2))

def manhattan_distance(x1:np.ndarray,x:np.ndarray)->int|float:
  """
Calculate the manhattan distance between two points.
Formula: d = sum(|x1_i - x2_i|)
"""

  return np.sum(np.abs(x1-x2))

def minkowaski_distance(x1:np.ndarray,x:np.ndarray)->int|float:
  '''
Calculate Minkowaski distance (generalised distance metric)
Formula d = (sum(|x1_i - x2_i|^p^(1/p))
p=1: Manhattan distance
p=2: Euclidean distance

'''
  return np.power(np.sum(np.abs(x1-x2)** p), 1/p)

"""# III. Data Pre-Processing Utilities"""

def train_test_split(X:np.ndarray, y:np.ndarray, test_size=0.2, random_state=None):
  if random_state is not None:
    np.random.seed(random_state)

  n_samples = len(X)
  n_test  = int(n_samples * test_size)

#Shuffling indicies
  indicies = np.random.permutation(n_samples)

  test_indicies = indicies[:n_test]
  train_indicies = indicies[n_test:]

  return X[train_indicies] ,y[train_indicies], X[test_indicies],y[test_indicies]

class LabelEncoder:
  def __init__(self) -> None:
    self.classes_ = {}
    self.inverse_classes_ = {}
  def fit(self, data:np.ndarray):
    unique_values = sorted(set(data))
    self.classes_ = {val:idx for idx, val in enumerate(unique_values)}
    self.classes_ = {idx: val for val, idx in enumerate(unique_values)}
    print("Class mapping:", self.classes_)

    self.inverse_classes_ = {idx:val for val, idx in self.classes_.items()}
    return self
  def transform(self, data:np.ndarray):
    return np.array([self.classes_[val] for val in data])
  def fit_transform(self, data:np.ndarray):
    self.fit(data)
    return self.transform(data)

class MinMaxScaler:
  def __init__(self):
    self.min = None
    self.max = None

  def fit(self,X):
    self.min_ = np.min(X, axis = 0)
    self.max_ = np.max(X, axis = 0)
    return self

  def transform(self, X):
    range_ = self.max_ -self.min_
    range_[range_ == 0] = 1 #Prevent zero division error
    return (X - self.min_) / range_

  def fit_transform(self,X):
    self.fit(X)
    return self.transform(X)

class StandardScaler:
  def __init__(self)-> None:
    self.mean_ = None
    self.std_ = None

  def fit(self,X):
    self.mean_ = np.mean(X, axis = 0)
    self.std_ = np.std(X, axis = 0)
    return self

  def transform(self,X):
    std = self.std_.copy()
    std[std == 0] = 1 #Prevent divison by zero
    return (X - self.mean_) / std

  def fit_transform(self,X):
    self.fit(X)
    return self.transform(X)

"""## IV. Building K-Proximity Neighbour"""

#non library based prediction - mathematical formulae

class KProximityNeighbour:
  '''
  Algorithms:
  1. Store training data (lazy learning - no explicit training)
  2. For each test point:
    a. calculate distance to all training points
    b. select K proximity neighbours
    c. perform majority vote
    d. assign most common class

  '''

  def __init__(self, k=5, distance_metric="euclidean", p=2, weighted=False) -> None:
    self.k = k
    self.distance_metric = distance_metric
    self.p = p
    self.weighted = weighted
    self.X_train = None
    self.y_train = None

  def _get_distance(self, x1:np.ndarray, x2:np.ndarray)->int|float:
    if self.distance_metric == "euclidean":
      return euclidean_distance(x1, x2)
    elif self.distance_metric == "manhattan":
      return manhattan_distance(x1, x2)
    elif self.distance_metric == "minkowski":
      return minkowaski_distance(x1, x2)
    else:
      raise ValueError(f"Unknown distance metric: {self.distance_metric}")

  def fit(self, X:Optional[np.ndarray],y:Optional[np.ndarray]):
    self.X_train = X
    self.y_train = y
    return self

  def _predict_single(self,x):
    #Calculate distance for a single sample
    distances = [self._get_distance(x, x_train) for x_train in self.X_train]

    #Get indicies of K-Nearest Neighbours
    k_indicies = np.argsort(distances)[:self.k] #values w the highest distances
    k_nearest_labels = self.y_train [k_indicies]
    k_nearest_distances = np.array(distances)[k_indicies]

    if self.weighted:
      #Distance - weighted voting
      #Weight = 1 / (distance + epsilon) to avoid division by zero
      epsilon = 1e-8
      weights = 1 / (k_nearest_distances + epsilon)

      #Weighted vote count
      label_weights = {}
      for label, weight in zip(k_nearest_labels, weights):
        label_weights[label] = label_weights.get(label, 0) + weight

      return max(label_weights, key=label_weights.get)
    else:
        #simple majority voting
        most_common = Counter(k_nearest_labels).most_common(1)
        return most_common[0][0] # returns first most common/near

  def predict(self, X:np.ndarray)->np.ndarray:
    X = np.array(X)
    return np.array([self._predict_single(x) for x in X])

  #predicting probsilities
  def predict_prob(self, X:np.ndarray)->np.ndarray:
    X = np.array(X)
    probabilities = []

    for x in X:
      distances = [self._get_distance(x, x_train) for x_train in self.X_train]
      k_indices = np.argsort(distances)[:self.k]
      k_nearest_labels = self.y_train[k_indices]

#Count the votes for each class
      unique_classes = np.unique(self.y_train)
      probs = []
      for cls in unique_classes:
        count = np.sum(k_nearest_labels == cls)
        probs.append(count/self.k)

        probabilities.append(probabilities)
    return np.array(probabilities)

"""# V. Evaluation Metrics"""

#Calculating the accuracy -  how many scores are actually correct

def accuracy_score(y_true:np.ndarray, y_pred:np.ndarray)->int|float:
  '''
  Calculate accuracy: correct predctions by total predictions
  '''
  return np.sum(y_true == y_pred) / len(y_true)

def confusion_matrix(y_true:np.ndarray, y_pred:np.ndarray):
  '''
  Generate confusion matrix
  Returns 2D array:
  [[TN, FP], #true negative, false positive
  [FN, TP]] #false negative, true positive
  '''

  classes = np.unique(np.concatenate([y_true, y_pred]))
  n_classes = len(classes)

  matrix = np.zeros((n_classes, n_classes), dtype=int)

  for true, pred in zip(y_true, y_pred):
    matrix[int(true), int(pred)] +=1

  return matrix



# -------------------------------
# Validation Test

# Test case with known outcome
y_true_test = np.array([0, 0, 1, 1])
y_pred_test = np.array([0, 1, 0, 1])

cm_test = confusion_matrix(y_true_test, y_pred_test)

print("Total samples:", len(y_true_test))
print("Sum of confusion matrix:", cm_test.sum())
print("Confusion Matrix:\n", cm_test)

def precision_score(y_true:np.ndarray, y_pred:np.ndarray)->np.ndarray:

  '''
  Precision = TP/(TP + FP)

  '''

  cm = confusion_matrix(y_true, y_pred)
  tp = cm[1,1]
  fp = cm[0,1]
  return tp/(tp+fp) if (tp + fp) > 0 else 0

def recall_score(y_true:np.ndarray, y_pred:np.ndarray)->np.ndarray:

  '''
  Precision = TP/(TP + FN)

  '''

  cm = confusion_matrix(y_true, y_pred)
  tp = cm[1,1]
  fn = cm[0,1]
  return tp/(tp+fn) if (tp + fn) > 0 else 0

def f1_score(y_true:np.ndarray, y_pred:np.ndarray)->np.ndarray:

  '''
  F1 = 2 * (precision * recall) / (precision + recall)

  '''

  prec = precision_score(y_true, y_pred)
  rec = recall_score(y_true, y_pred)
  return 2 * (prec * rec)/(prec + rec) if (prec + rec) > 0 else 0

def classification_report(y_true:np.ndarray, y_pred:np.ndarray):
  '''
    Generate a classification report
  '''
  acc = accuracy_score(y_true, y_pred)
  prec = precision_score(y_true, y_pred)
  rec = recall_score(y_true, y_pred)
  f1 = f1_score(y_true, y_pred)

  report = f"""
╔══════════════════════════════════════╗
║      Classification Report           ║
╠══════════════════════════════════════╣
║  Accuracy:   {acc:.4f}               ║
║  Precision:  {prec:.4f}              ║
║  Recall:     {rec:.4f}               ║
║  F1 Score:   {f1:.4f}                ║
╚══════════════════════════════════════╝
  """

  return report

"""# VI. Main Execution"""

def dataset_loader()->pd.DataFrame:
  print(f"{YELLOW}" + "=" * 60)
  print(f"{TEAL} KNN Classification from Scratch-Heart Disease Datset")
  print(f"{YELLOW}" + "=" * 60)

  #Load data
  file_path = "/content/drive/MyDrive/heart.csv"
  df = pd.read_csv(file_path)

#Identify categorical columns
  categorical_cols = ["Sex", "ChestPainType", "RestingECG", "ExerciseAngina", "ST_Slope"]
  numerical_cols = ["Age", "RestingBP", "Cholesterol", "FastingBS", "MaxHR", "Oldpeak"]

  print(f"{BLUE}\n Categorical Features: {categorical_cols}")
  print(f"{CYAN}\n Numerical Faetures: {numerical_cols}")

  return df

def preprocess_transform(df:pd.DataFrame)->Dict:
  categorical_cols = ["Sex", "ChestPainType", "RestingECG", "ExerciseAngina", "ST_Slope"]
  numerical_cols = ["Age", "RestingBP", "Cholesterol", "FastingBS", "MaxHR", "Oldpeak"]
#Encode categorical features
  print(f"{WHITE}Encoding categorical features:-")
  encoders = {}
  for col in categorical_cols:
    encoders[col] = LabelEncoder()
    df[col] = encoders[col].fit_transform(df[col])

  #Separate features and target - MOVED OUTSIDE THE LOOP
  X = df.drop("HeartDisease", axis=1).values
  y = df["HeartDisease"].values

  #Split Data - MOVED OUTSIDE THE LOOP
  print(f"{PURPLE}Splitting data (80%, Train, 20% Test)")
  # FIX: Correctly unpack the return values from train_test_split
  X_train, y_train, X_test, y_test = train_test_split(X,y, test_size=0.2, random_state=42)
  print(f"{BLUE}Training Samples: {len(X_train)}")
  print(f"{CYAN}Testing Samples: {len(X_test)}")

  print("Scaling features using Min-MaxNormalisation")
  scaler = MinMaxScaler()
  X_trained_scaled = scaler.fit_transform(X_train)
  X_test_scaled = scaler.transform(X_test)

  #Final optimal K using cross validation approach

  print(f"{YELLOW}" + "=" * 60)
  print(f"{BLUE} Finding Optimal K Value")
  print(f"{YELLOW}" + "=" * 60)


  k_values = range(1,21)
  accuracies = []

  for k in k_values:
    knn = KProximityNeighbour(k=k, distance_metric="euclidean")
    knn.fit(X_trained_scaled, y_train)
    y_pred = knn.predict(X_test_scaled)
    acc = accuracy_score(y_test,y_pred)
    accuracies.append(acc)
    print(f"{TEAL} K={k:2d}: Accuracy = {acc:.4f} {'*' * int(acc * 20)}")


  best_k = k_values[np.argmax(accuracies)]
  best_accuracy = max(accuracies)
  print(f"{GREEN}Best K={best_k} with accuracy = {best_accuracy:.4f}")

  #Train final model with best K
  print(f"{YELLOW}" + "\n" + "=" * 60)
  print(f"Final Model (K={best_k})")
  print(f"{YELLOW}" + "=" * 60)

  knn_final = KProximityNeighbour(k=best_k, distance_metric="euclidean")
  knn_final.fit(X_trained_scaled, y_train)
  y_pred_final = knn_final.predict(X_test_scaled)

  return {"model": knn_final, "y_test_pred": y_pred_final}

df = dataset_loader()

display(df)

model_dict = preprocess_transform(df)